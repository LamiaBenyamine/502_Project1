---
title: "ST502: Project 1"
author: "Ming Cai & Lamia Benyamine"
date: "2024-10-08"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

# INTRODUCTION

There are many methods to approximate the probability of success, p,
from a Binomial random sample, however, not all have the same coverage
probabilities or average length of the confidence interval. This report
will review several different methods using R and compare the
performance of the confidence intervals for various sample sizes and
probabilities for these three properties:

-   The proportion of intervals that capture the true value

-   The proportion that miss above and proportion that miss below

-   The average length of the interval

We will use data collected from 1,00 random samples from a binomial
distribution where n varies from 15, 13, to 100 and p varies from 0.01
to 0.99 across each approximation method. The methods reviewed in this
report are:

-   Wald interval

-   Adjusted Wald interval

-   Clopper-Pearson (exact) interval

-   Score interval

-   Raw percentile interval using a parametric bootstrap

-   Bootstrap t interval using a parametric bootstrap.

# SIMULATION METHODS

***Note: We will use*** $\alpha = 0.05$ ***as default in all
calculations.*** Also, ***for all methods, when*** $y = 0$***, we set
the interval to be*** $[0,0]$ ***and when*** $y = n$***, we set the
interval to be*** $[1,1]$***.***

## Wald Interval

The formula of calculating Wald interval is:

$$
\hat{p} - z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} < p < \hat{p} + z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}},
$$

where the $z_c$ denotes the $c$ quantile of the standard normal
distribution. To calculate the Wald interval for $p$, we need to use
sample's probability $\hat{p}$, sample size $n$ and the confidence level
$1-\alpha$.

The function for calculating the Wald Interval is given below:

```{r}
WaldCI <- function(y, n, alpha = 0.05){
 # Initialize vectors to store lower and upper intervals
  lower_intervals <- numeric(length(y))
  upper_intervals <- numeric(length(y))
 
  # Loop through each success value
  for (i in seq_along(y)) {
      # Estimate of the probability of success
      p_hat = y[i]/n  
    # Special cases for y = 0 or y = n
    if (y[i] == 0) {
      lower_intervals[i] <- 0
      upper_intervals[i] <- 0  
    } else if (y[i] == n) {
      lower_intervals[i] <- 1
      upper_intervals[i] <- 1
    } else {
      # Calculate the Wald CI
      lower_intervals[i] <- p_hat - qnorm(1-alpha/2)*sqrt((p_hat*(1-p_hat))/n)
      upper_intervals[i] <- p_hat + qnorm(1-alpha/2)*sqrt((p_hat*(1-p_hat))/n)
    }
  }
  # Return the lists of lower and upper intervals
  return(list(Lower = lower_intervals, Upper = upper_intervals))
}
```

## Adjusted Wald interval

To overcome the poor performance of Wald interval, adjusted Wald
interval was proposed. The formula of adjusted Wald interval is almost
the same as Wald interval, except that we add 2 successes and 2 failures
to the sample data set:

$$
\tilde{p} - z_{1-\frac{\alpha}{2}}\sqrt{\frac{\tilde{p}(1-\tilde{p})}{n}} < p < \tilde{p} + z_{1-\frac{\alpha}{2}}\sqrt{\frac{\tilde{p}(1-\tilde{p})}{n}},
$$

where $\tilde{p} = (Y+2)/(n+4)$.

The function for calculating the Adjusted Wald Interval is given below:

```{r}
AdjWaldCI <- function(y, n, alpha = 0.05) {
 # Initialize vectors to store lower and upper intervals
  lower_intervals <- numeric(length(y))
  upper_intervals <- numeric(length(y))
  
  # Loop through each success value
  for (i in seq_along(y)) {
    # Calculate adjusted estimate of probability of success
    p_tilde <- (y[i] + 2) / (n + 4)

    # Special cases for y = 0 or y = n
    if (y[i] == 0) {
      lower_intervals[i] <- 0
      upper_intervals[i] <- 0  
    } else if (y[i] == n) {
      lower_intervals[i] <- 1
      upper_intervals[i] <- 1
    } else {
      # Calculate the adjusted Wald CI
      lower_intervals[i] <- p_tilde - qnorm(1 - alpha / 2) * sqrt((p_tilde * (1 - p_tilde)) / (n + 4))
      upper_intervals[i] <- p_tilde + qnorm(1 - alpha / 2) * sqrt((p_tilde * (1 - p_tilde)) / (n + 4))
    }
  }
  # Return the lists of lower and upper intervals
  return(list(Lower = lower_intervals, Upper = upper_intervals))
}
```

## Clopper-Pearson (exact) interval

Clopper-Pearson interval is referred to as an "exact" interval. This
interval is guaranteed to have coverage probability of at least
$1 - \alpha$ for every value of $p$. When $y = 1,2, \cdots , n - 1$, the
confidence interval is:

$$
\left[ 1 + \frac{n-y+1}{yF_{2y,2(n-y+1),\frac{\alpha}{2}}} \right]^{-1} < p < \left[ 1 + \frac{n-y}{(y+1)F_{2(y+1),2(n-y),1-\frac{\alpha}{2}}} \right]^{-1},
$$

where the $F_{a,b,c}$ denotes the $c$ quantile from the F distribution
with degrees of freedom $a$ and $b$.

The function for calculating the Clopper-Pearson interval is given
below:

```{r}
ClopperPearsonCI <- function(y, n, alpha = 0.05){
 # Initialize vectors to store lower and upper intervals
  lower_intervals <- numeric(length(y))
  upper_intervals <- numeric(length(y))
 
  # Loop through each success value
  for (i in seq_along(y)) {
    
   # Special cases for y = 0 or y = n
    if (y[i] == 0) {
      lower_intervals[i] <- 0
      upper_intervals[i] <- 0
    } else if (y[i] == n) {
      lower_intervals[i] <- 1
      upper_intervals[i] <- 1
    } else {
      # Calculate the general case for Clopper-Pearson CI using F-distribution
      lower_intervals[i] <- 1 / (1 + ((n-y[i]+1) / (y[i] * qf(alpha/2, 2*y[i], 2*(n-y[i]+1))))) 
      upper_intervals[i] <- 1 / (1 + ((n-y[i]) / ((y[i]+1) * qf(1-(alpha/2), 2*(y[i]+1), 2*(n-y[i])))))
    }
  }
 # Return the lists of lower and upper intervals
  return(list(Lower = lower_intervals, Upper = upper_intervals))
}
```

## Score interval

Score tests, and in particular their standard errors, are based on the
log-likelihood at the null hypothesis value of the parameter, whereas
Wald tests are based on the log-likelihood at the maximum likelihood
estimate. Score interval can be used with almost all sample sizes and
$p$ values. The formula is given below:

$$
\frac{\hat{p}+\frac{z_{1-\frac{\alpha}{2}}^2}{2n}-z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p}) + \frac{z_{1-\frac{\alpha}{2}}^2}{4n}}{n}}}{1+\frac{z_{1-\frac{\alpha}{2}}^2}{n}} < p < \frac{\hat{p}+\frac{z_{1-\frac{\alpha}{2}}^2}{2n}+z_{1-\frac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p}) + \frac{z_{1-\frac{\alpha}{2}}^2}{4n}}{n}}}{1+\frac{z_{1-\frac{\alpha}{2}}^2}{n}},
$$

where the $z_c$ denotes the $c$ quantile of the standard normal
distribution.

The function for calculating the Score interval is given below:

```{r}
ScoreCI <- function(y, n, alpha = 0.05){
  # Z-score for the desired confidence level
  z = qnorm(1-alpha/2) 
  # Initialize vectors to store lower and upper intervals
  lower_intervals <- numeric(length(y))
  upper_intervals <- numeric(length(y))
 
  # Loop through each success value
  for (i in seq_along(y)) {
    # Estimate of the probability of success
    p_hat = y[i]/n  
    # Special cases for y = 0 or y = n
    if (y[i] == 0) {
      lower_intervals[i] <- 0
      upper_intervals[i] <- 0  
    } else if (y[i] == n) {
      lower_intervals[i] <- 1
      upper_intervals[i] <- 1
    } else {
      # Calculate the Score CI
      lower_intervals[i] <- 
      (p_hat + z^2/(2*n) - z*sqrt((p_hat*(1-p_hat) + z^2/(4*n))/n)) / (1 + z^2/n)
      upper_intervals[i] <-
      (p_hat + z^2/(2*n) + z*sqrt((p_hat*(1-p_hat) + z^2/(4*n))/n)) / (1 + z^2/n)
    }
  }
  # Return the lists of lower and upper intervals
  return(list(Lower = lower_intervals, Upper = upper_intervals))
}
```

## Raw percentile interval and Bootstrap t interval function (both using a parametric bootstrap)

To find the Raw percentile interval, we need to find the
$\frac{\alpha}{2}$ quantile and the $1-\frac{\alpha}{2}$ quantile of the
$\hat{p}$ of samples.

$$
\hat{p}_{\frac{\alpha}{2}} < p < \hat{p}_{1-\frac{\alpha}{2}} 
$$

where the $\hat{p}_{c}$ denotes the $c$ quantile of the proportion of
the samples.

To find the Bootstrap t interval, we use following steps:

1.  Calculate the mean of our sample's $\hat{p}$ to generate
    bootstrapped $\hat{p}_{boot}$,

2.  Use the mean of bootstrapped $\hat{p}_{boot}$ to generate secondary
    bootstrapped $\hat{p}_{boot2}$,

3.  Use secondary bootstrapped $\hat{p}_{boot2}$ to find the estimated
    standard error of $\hat{p}_{boot}$,

4.  Calculate the interval based on formula.

$$
\hat{p} - t_{1-{\frac{\alpha}{2}}}\cdot \hat{SE} < p < \hat{p} - t_{\frac{\alpha}{2}}\cdot \hat{SE}
$$

where the $t_{c}$ denotes the $c$ quantile of the bootstrapped t values,
and

$$
T = \frac{\underset{\sim}{\hat{p}}-p}{\underset{\sim}{\hat{SE}}(\underset{\sim}{\hat{p}})}
$$

The function for finding the Raw percentile interval and the Bootstrap t
interval is given below:

```{r}
BootstrapCI <- function(y, n, B, alpha = 0.05) {
  # Initialize vectors to store lower and upper intervals
  raw_lower <- numeric(length(y))
  raw_upper <- numeric(length(y))
  boot_t_interval_low <- numeric(length(y))
  boot_t_interval_up <- numeric(length(y))

  # Loop through each success value
  for (i in seq_along(y)) {
    # Estimate the proportion
    p_hat <- y[i] / n

    # Bootstrap t interval calculations
    set.seed(4321)
    boot_estimates <- replicate(B, {
      sim_data <- rbinom(1, n, prob = p_hat)  # Generate bootstrap sample
      p_hat_boot <- sim_data / n  # Bootstrap estimate of proportion

      # Perform secondary bootstrap for standard error estimation
      B2 <- 50
      secondary_boot <- replicate(B2, {
        sim_data2 <- rbinom(1, n, prob = p_hat_boot)  # Second bootstrap sample
        p_hat_boot2 <- sim_data2 / n  # Bootstrap estimate
        return(p_hat_boot2)
      })

      # Find SE for t-stat
      estimated_SE_p_hat_boot <- sd(secondary_boot)  # Standard error
      if (estimated_SE_p_hat_boot == 0) return(c(NA, NA))  # Handle division by zero

      # Calculate t-statistic
      p_boot_t <- (p_hat_boot - p_hat) / estimated_SE_p_hat_boot
      return(c(p_hat_boot, p_boot_t))
    })

    # Calculate raw percentile interval from bootstrap samples
    raw_lower[i] <- quantile(boot_estimates[1, ], alpha / 2, na.rm = TRUE)
    raw_upper[i] <- quantile(boot_estimates[1, ], 1 - alpha / 2, na.rm = TRUE)

    # Calculate the t-bootstrap intervals using quantiles
    boot_t_interval_low[i] <- p_hat - quantile(boot_estimates[2, ], 1 - alpha / 2, na.rm = TRUE) * sd(boot_estimates[1, ])
    boot_t_interval_up[i] <- p_hat - quantile(boot_estimates[2, ], alpha / 2, na.rm = TRUE) * sd(boot_estimates[1, ])
  }
  
  # combine intervals into one list
  raw_interval <- list(Lower = raw_lower, Upper = raw_upper)
  t_interval <- list(Lower = boot_t_interval_low, Upper = boot_t_interval_up)

  # Return both raw percentile and t-bootstrap intervals
  return(list(raw_boot = raw_interval, t_boot = t_interval))
}
```

## Creation of Data

We generated $N = 1000$ random samples from a binomial distribution
where sample size $n$ varies across 15, 30, to 100 and true proportion
$p$ varies from 0.01 to 0.99 (15 total values of p).

```{r}
# Set values
N <- 1000 # Number of random samples
n <- c(15, 30, 100) # Values of n
p <- seq(0.01, 0.99, length.out = 15) # Values of p

# Set seed to allow for replication of data
set.seed(4321) 
# Function to generate random binomial sample data for each combination of n and p
GetData <- function(n_vals, p_vals, N = N) { 
  # Initialize a list to store sample data
  sample_data <- list()  
  for (n in n_vals) {
    for (p in p_vals) {
      samples <- rbinom(N, size = n, prob = p)  # Generate N binomial samples
      col_name <- paste0("n", n, "p", p)  # Create column name for the dataset
      sample_data[[col_name]] <- samples  # Store samples in the list
    }
  }
  return(sample_data)
}
sample_data <- GetData(n, p, N)
```

## Execute the Monte Carlo simulation

### Wald Simulation

```{r}
# Initialize a data frame to store the results
waldci_results <- data.frame()

for (col_name in names(sample_data)) {
    samples <- sample_data[[col_name]]  # Get the samples
    # Extract the n and p values from the column name
    n_value <- as.numeric(sub("n(\\d+)p.*", "\\1", col_name))
    true_p <- as.numeric(sub(".*p(\\d+\\.\\d+)", "\\1", col_name))
    
    # Compute the CI
    ci <- WaldCI(samples, n_value)
    
    # Store results
    waldci_results <- rbind(waldci_results, data.frame(
      Method = "Wald",
      Sample = col_name,
      True_P = true_p,
      Size = n_value,
      # Check if the true p value is within the CI
      Coverage_rate = mean((ci[[1]] <= true_p) & (ci[[2]] >= true_p)), 
      Miss_Above = mean((ci[[2]] < true_p)),  # True p is above CI
      Miss_Below = mean((ci[[1]] > true_p)),   # True p is below CI
      Width = mean(ci[[2]] - ci[[1]]) # Width of interval
  ))
}
# Aggregate by taking the average of the properties 
Wald_Properties <- aggregate(cbind(Coverage_rate, Miss_Above, Miss_Below, Width) ~ Size, data = waldci_results, FUN = mean)
```

### Adjusted Wald Simulation

```{r}
# Initialize a data frame to store the results
adjwaldci_results <- data.frame()

for (col_name in names(sample_data)) {
    samples <- sample_data[[col_name]]  # Get the samples
    # Extract the n and p values from the column name
    n_value <- as.numeric(sub("n(\\d+)p.*", "\\1", col_name))
    true_p <- as.numeric(sub(".*p(\\d+\\.\\d+)", "\\1", col_name))
    
    # Compute the CI
    ci <- AdjWaldCI(samples, n_value)
    
    # Store results
    adjwaldci_results <- rbind(adjwaldci_results, data.frame(
      Method = "ADJWald",
      Sample = col_name,
      True_P = true_p,
      Size = n_value,
      # Check if the true p value is within the CI
      Coverage_rate = mean((ci[[1]] <= true_p) & (ci[[2]] >= true_p)), 
      Miss_Above = mean((ci[[2]] < true_p)),  # True p is above CI
      Miss_Below = mean((ci[[1]] > true_p)),   # True p is below CI
      Width = mean(ci[[2]] - ci[[1]]) # Width of interval
  ))
}
# Aggregate by taking the average of the properties 
Adj_Wald_Properties <- aggregate(cbind(Coverage_rate, Miss_Above, Miss_Below, Width) ~ Size, data = adjwaldci_results, FUN = mean)
```

### Results from Clopper Pearson Simulation

```{r}
# Initialize a data frame to store the results
cpci_results <- data.frame()

for (col_name in names(sample_data)) {
    samples <- sample_data[[col_name]]  # Get the samples
    # Extract the n and p values from the column name
    n_value <- as.numeric(sub("n(\\d+)p.*", "\\1", col_name))
    true_p <- as.numeric(sub(".*p(\\d+\\.\\d+)", "\\1", col_name))
    
    # Compute the CI
    ci <- ClopperPearsonCI(samples, n_value)
    
   # Store results
    cpci_results <- rbind(cpci_results, data.frame(
      Method = "CP",
      Sample = col_name,
      True_P = true_p,
      Size = n_value,
      # Check if the true p value is within the CI
      Coverage_rate = mean((ci[[1]] <= true_p) & (ci[[2]] >= true_p)), 
      Miss_Above = mean((ci[[2]] < true_p)),  # True p is above CI
      Miss_Below = mean((ci[[1]] > true_p)),   # True p is below CI
      Width = mean(ci[[2]] - ci[[1]]) # Width of interval
  ))
}
# Aggregate by taking the average of the properties 
CP_Properties <- aggregate(cbind(Coverage_rate, Miss_Above, Miss_Below, Width) ~ Size, data = cpci_results, FUN = mean)
```

### Results from Score Interval Simulation

```{r}
# Initialize a data frame to store the results
scoreci_results <- data.frame()

# Loop through each sample in sample_data to compute CIs
for (col_name in names(sample_data)) {
  samples <- sample_data[[col_name]]  # Get the samples
  # Extract the n and p values from the column name
  n_value <- as.numeric(sub("n(\\d+)p.*", "\\1", col_name))
  true_p <- as.numeric(sub(".*p(\\d+\\.\\d+)", "\\1", col_name))
  
  # Compute the CI
  ci <- ScoreCI(samples, n_value)
  
  # Store results
  scoreci_results <- rbind(scoreci_results, data.frame(
    Method = "Score",
    Sample = col_name,
    True_P = true_p,
    Size = n_value,
    # Check if the true p value is within the CI
    Coverage_rate = mean((ci[[1]] <= true_p) & (ci[[2]] >= true_p)),
    Miss_Above = mean((ci[[2]] < true_p)),  # True p is above CI
    Miss_Below = mean((ci[[1]] > true_p)),   # True p is below CI
    Width = mean(ci[[2]] - ci[[1]]) # Width of interval
  ))
}
# Aggregate by taking the average of the properties 
Score_Properties <- aggregate(cbind(Coverage_rate, Miss_Above, Miss_Below, Width) ~ Size, data = scoreci_results, FUN = mean)
```

### Results from Bootstrap Interval Simulation (Raw percentile & t-interval)

```{r}
# Initialize a data frame to store the results and set the replication values
raw_bootci_results <- data.frame()
t_bootci_results <- data.frame()
B = 100
B2 = 50

# Loop through each sample in sample_data to compute CIs
for (col_name in names(sample_data)) {
  samples <- sample_data[[col_name]]  # Get the samples
  # Extract the n and p values from the column name
  n_value <- as.numeric(sub("n(\\d+)p.*", "\\1", col_name))
  true_p <- as.numeric(sub(".*p(\\d+\\.\\d+)", "\\1", col_name))
  
  # Compute the CI
  ci <- BootstrapCI(samples, n_value, B)

    # Store results for raw percentile bootstrap method
  raw_bootci_results <- rbind(raw_bootci_results, data.frame(
    Method = "Raw-boot",
    Sample = col_name,
    True_P = true_p,
    Size = n_value,
    # Check if the true p value is within the CI
    Coverage_rate = mean((ci[[1]][[1]] <= true_p) & (ci[[1]][[2]] >= true_p)), 
    Miss_Above = mean((ci[[1]][[2]] < true_p)),  # True p is above CI
    Miss_Below = mean((ci[[1]][[1]] > true_p)),   # True p is below CI
    Width = mean(ci[[1]][[2]] - ci[[1]][[1]]) # Width of interval
  ))
  
    # Store results for t-interval bootstrap method
  t_bootci_results <- rbind(t_bootci_results, data.frame(
    Method = "t-interval",
    Sample = col_name,
    True_P = true_p,
    Size = n_value,
    # Check if the true p value is within the CI
    Coverage_rate = mean((ci[[2]][[1]] <= true_p) & (ci[[2]][[2]] >= true_p)), 
    Miss_Above = mean((ci[[2]][[2]] < true_p)),  # True p is above CI
    Miss_Below = mean((ci[[2]][[1]] > true_p)),   # True p is below CI
    Width = mean(ci[[2]][[2]] - ci[[2]][[1]]) # Width of interval
  ))
}
# Aggregate by taking the average of the properties 
Raw_Boot_Properties <- aggregate(cbind(Coverage_rate, Miss_Above, Miss_Below, Width) ~ Size, data = raw_bootci_results, FUN = mean)
t_Interval_Boot_Properties <- aggregate(cbind(Coverage_rate, Miss_Above, Miss_Below, Width) ~ Size, data = t_bootci_results, FUN = mean)
```

# RESULTS

Combining the results from the properties of all the above methods.

```{r}
# Add an attribute column
Wald_Properties$Method <- "Wald"
Adj_Wald_Properties$Method <- "AdjWald"
CP_Properties$Method <- "CP"
Score_Properties$Method <- "Score"
Raw_Boot_Properties$Method <- "Raw-% boot"
t_Interval_Boot_Properties$Method <- "t-int boot"

# Combine all tables into one
combined_results <- arrange(rbind(Wald_Properties, Adj_Wald_Properties, CP_Properties, Score_Properties, Raw_Boot_Properties, t_Interval_Boot_Properties), Size)

library(tidyverse)
pivoted_results <- 
  combined_results |>
  arrange(Size, Coverage_rate)
```

# CONCLUSION
